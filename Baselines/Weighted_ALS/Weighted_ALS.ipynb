{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set your data directory path\n",
    "DATA_DIR = '../../Data/'\n",
    "ratings_path = os.path.join(DATA_DIR, 'train_ratings.csv')\n",
    "wishlist_path = os.path.join(DATA_DIR, 'train_tbr.csv')\n",
    "sample_path = os.path.join(DATA_DIR, 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Alternating Least Squares (ALS) for implicit feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tbr_data(train_path):\n",
    "    \"\"\"\n",
    "    Load TBR (wishlist) CSV which already has sid,pid columns.\n",
    "    \"\"\"\n",
    "    implicit_df = pd.read_csv(train_path)\n",
    "    implicit_df[\"sid\"] = implicit_df[\"sid\"].astype(int)\n",
    "    implicit_df[\"pid\"] = implicit_df[\"pid\"].astype(int)\n",
    "    # add a flag column for convenience:\n",
    "    implicit_df[\"tbr_flag\"] = 1\n",
    "    return implicit_df\n",
    "\n",
    "def load_data(train_path):\n",
    "    \"\"\"\n",
    "    Load ratings CSV, split sid_pid into sid and pid, drop original column.\n",
    "    \"\"\"\n",
    "    ratings = pd.read_csv(train_path)\n",
    "    ratings[[\"sid\", \"pid\"]] = ratings[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    ratings.drop(columns=[\"sid_pid\"], inplace=True)\n",
    "    ratings[\"sid\"] = ratings[\"sid\"].astype(int)\n",
    "    ratings[\"pid\"] = ratings[\"pid\"].astype(int)\n",
    "    return ratings\n",
    "\n",
    "def read_data_matrix(df):\n",
    "    \"\"\"Returns matrix view of the training data, where rows are scientists (sid) and\n",
    "    columns are papers (pid).\"\"\"\n",
    "    return df.pivot(index=\"sid\", columns=\"pid\", values=\"rating\").fillna(0)\n",
    "\n",
    "def make_submission(model, sample_path, filename):\n",
    "    df_sub = pd.read_csv(sample_path)\n",
    "    sid_pid = df_sub[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    sids = sid_pid[0].astype(int).values\n",
    "    pids = sid_pid[1].astype(int).values\n",
    "    df_sub[\"rating\"] = model.predict_for_submission(sids, pids)\n",
    "    df_sub.to_csv(filename, index=False)\n",
    "    print(f\"Submission saved to {filename}\")\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return math.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted ALS Model Implementation (with Implicit ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedALSModel:\n",
    "    def __init__(self,\n",
    "                 rank=100,\n",
    "                 num_iterations=10,\n",
    "                 reg_parameter=0.1,\n",
    "                 num_svd_runs=3,\n",
    "                 svd_lr=0.1,\n",
    "                 use_iSVD=False,\n",
    "                 transpose=False,\n",
    "                 bias_reg=0.01,\n",
    "                 use_bias=True,\n",
    "                 use_confidence=True,\n",
    "                 alpha_r=40.0,\n",
    "                 alpha_tbr=10.0):\n",
    "        \"\"\"\n",
    "        Extended ALS with optional bias terms and two‚Äêsignal confidence weighting.\n",
    "        \"\"\"\n",
    "        self.rank           = rank\n",
    "        self.num_iters      = num_iterations\n",
    "        self.lam            = reg_parameter\n",
    "        self.num_svd_runs   = num_svd_runs\n",
    "        self.lr             = svd_lr\n",
    "        self.use_iSVD       = use_iSVD\n",
    "        self.transpose      = transpose\n",
    "        self.bias_reg       = bias_reg\n",
    "        self.use_bias       = use_bias\n",
    "        self.use_confidence = use_confidence\n",
    "        self.alpha_r        = alpha_r\n",
    "        self.alpha_tbr      = alpha_tbr\n",
    "\n",
    "        # to be filled after training\n",
    "        self.rec_mtx     = None\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.global_mean  = None\n",
    "        self.user_bias    = None\n",
    "        self.item_bias    = None\n",
    "\n",
    "    def extract_data(self, df):\n",
    "        return df[\"sid\"].values, df[\"pid\"].values, df[\"rating\"].values\n",
    "\n",
    "    def calculate_confidence(self, rating, tbr_flag):\n",
    "        \"\"\"\n",
    "        c_ui = 1 + alpha_r * r_ui + alpha_tbr * 1{tbr}\n",
    "        \"\"\"\n",
    "        return 1.0 + self.alpha_r * rating + self.alpha_tbr * tbr_flag\n",
    "\n",
    "    def proj_SVD(self, A, mask, lr, rank, num_iters):\n",
    "        U = np.random.uniform(-1,1,(A.shape[0], rank))\n",
    "        V = np.random.uniform(-1,1,(rank, A.shape[1]))\n",
    "        A_curr = np.zeros_like(A)\n",
    "        for _ in range(num_iters):\n",
    "            diff = (A - A_curr) * mask\n",
    "            pre   = A_curr + lr * diff\n",
    "            u, s, vt = svds(pre, k=rank)\n",
    "            idx = np.argsort(s)[::-1]\n",
    "            s, u, vt = s[idx], u[:,idx], vt[idx]\n",
    "            S = np.diag(s)\n",
    "            U = u.dot(np.sqrt(S))\n",
    "            V = np.sqrt(S).dot(vt)\n",
    "            A_curr = U.dot(V)\n",
    "        return U, V, A_curr\n",
    "\n",
    "    def iSVD(self, A, mask, rank, num_iters):\n",
    "        U = np.random.uniform(-1,1,(A.shape[0], rank))\n",
    "        V = np.random.uniform(-1,1,(rank, A.shape[1]))\n",
    "        A_curr = A.copy()\n",
    "        for _ in range(num_iters):\n",
    "            u, s, vt = svds(A_curr, k=rank)\n",
    "            idx = np.argsort(s)[::-1]\n",
    "            s, u, vt = s[idx], u[:,idx], vt[idx]\n",
    "            S = np.diag(s)\n",
    "            U = u.dot(np.sqrt(S))\n",
    "            V = np.sqrt(S).dot(vt)\n",
    "            A_curr = A * mask + (U.dot(V)) * (1-mask)\n",
    "        return U, V, A_curr\n",
    "\n",
    "    def ALS(self, users, items, ratings, tbr_flags):\n",
    "        # dimension\n",
    "        n_users = users.max()+1\n",
    "        n_items = items.max()+1\n",
    "\n",
    "        # build data + mask\n",
    "        data = np.zeros((n_users, n_items))\n",
    "        mask = np.zeros((n_users, n_items))\n",
    "        for u,i,r in zip(users, items, ratings):\n",
    "            data[u,i] = r\n",
    "            mask[u,i] = 1\n",
    "\n",
    "        # build confidence\n",
    "        if self.use_confidence:\n",
    "            C = np.ones((n_users, n_items))\n",
    "            for u,i,r,f in zip(users, items, ratings, tbr_flags):\n",
    "                C[u,i] = self.calculate_confidence(r, f)\n",
    "        else:\n",
    "            C = mask.copy()\n",
    "\n",
    "        # global & bias\n",
    "        global_mean = np.nanmean(np.where(mask, data, np.nan))\n",
    "        user_mean   = np.nan_to_num(np.nanmean(np.where(mask, data, np.nan), axis=1), nan=global_mean)\n",
    "        item_mean   = np.nan_to_num(np.nanmean(np.where(mask, data, np.nan), axis=0), nan=global_mean)\n",
    "        if self.use_bias:\n",
    "            ubias = user_mean - global_mean\n",
    "            ibias = item_mean - global_mean\n",
    "        else:\n",
    "            ubias = np.zeros(n_users)\n",
    "            ibias = np.zeros(n_items)\n",
    "\n",
    "        # subtract bias\n",
    "        A = data.copy()\n",
    "        if self.use_bias:\n",
    "            for u in range(n_users):\n",
    "                for i in range(n_items):\n",
    "                    if mask[u,i]:\n",
    "                        A[u,i] = data[u,i] - global_mean - ubias[u] - ibias[i]\n",
    "\n",
    "        # init factors\n",
    "        if self.use_iSVD:\n",
    "            U, V, _ = self.iSVD(A, mask, self.rank, self.num_svd_runs)\n",
    "        else:\n",
    "            U, V, _ = self.proj_SVD(A, mask, self.lr, self.rank, self.num_svd_runs)\n",
    "\n",
    "        # ALS loops\n",
    "        I_r = np.eye(self.rank)\n",
    "        for _ in range(self.num_iters):\n",
    "            # update V (items)\n",
    "            UtU = U.T.dot(U)\n",
    "            for j in range(n_items):\n",
    "                idx_u = np.where(mask[:,j]==1)[0]\n",
    "                if idx_u.size:\n",
    "                    Cj = C[idx_u,j]\n",
    "                    Uj = U[idx_u,:]\n",
    "                    Aj = A[idx_u,j]\n",
    "                    A_mat = UtU + Uj.T.dot(np.diag(Cj-1)).dot(Uj) + self.lam*I_r\n",
    "                    b_vec = Uj.T.dot(Cj*Aj)\n",
    "                    V[:,j] = np.linalg.solve(A_mat, b_vec)\n",
    "\n",
    "            # update U (users)\n",
    "            VtV = V.dot(V.T)\n",
    "            for u in range(n_users):\n",
    "                idx_i = np.where(mask[u,:]==1)[0]\n",
    "                if idx_i.size:\n",
    "                    Ci = C[u,idx_i]\n",
    "                    Vi = V[:,idx_i]\n",
    "                    Ai = A[u,idx_i]\n",
    "                    B_mat = VtV + Vi.dot(np.diag(Ci-1)).dot(Vi.T) + self.lam*I_r\n",
    "                    d_vec = Vi.dot(Ci*Ai)\n",
    "                    U[u,:] = np.linalg.solve(B_mat, d_vec)\n",
    "\n",
    "            # update biases\n",
    "            if self.use_bias:\n",
    "                for u in range(n_users):\n",
    "                    idx_i = np.where(mask[u,:]==1)[0]\n",
    "                    res = data[u,idx_i] - global_mean - ibias[idx_i] - U[u,:].dot(V[:,idx_i])\n",
    "                    ubias[u] = res.sum() / (idx_i.size + self.bias_reg)\n",
    "                for j in range(n_items):\n",
    "                    idx_u = np.where(mask[:,j]==1)[0]\n",
    "                    res = data[idx_u,j] - global_mean - ubias[idx_u] - (U[idx_u,:]*V[:,j]).sum(axis=1)\n",
    "                    ibias[j] = res.sum() / (idx_u.size + self.bias_reg)\n",
    "\n",
    "        # build full prediction matrix\n",
    "        rec = np.zeros((n_users, n_items))\n",
    "        for u in range(n_users):\n",
    "            for i in range(n_items):\n",
    "                rec[u,i] = global_mean + ubias[u] + ibias[i] + U[u,:].dot(V[:,i])\n",
    "        rec = np.clip(rec, 1, 5)\n",
    "\n",
    "        # save for later\n",
    "        self.rec_mtx      = rec\n",
    "        self.user_factors = U\n",
    "        self.item_factors = V\n",
    "        self.global_mean  = global_mean\n",
    "        self.user_bias    = ubias\n",
    "        self.item_bias    = ibias\n",
    "\n",
    "        return rec\n",
    "\n",
    "    def train(self, train_data, tbr_data=None, test_data=None):\n",
    "        \"\"\"\n",
    "        train_data: DataFrame with ['sid','pid','rating']\n",
    "        tbr_data:   DataFrame with ['sid','pid','tbr_flag']\n",
    "        \"\"\"\n",
    "        # explicit\n",
    "        u,i,r = self.extract_data(train_data)\n",
    "        # tbr flags array\n",
    "        if tbr_data is not None:\n",
    "            df = train_data.merge(tbr_data, on=[\"sid\",\"pid\"], how=\"left\").fillna(0)\n",
    "            t = df[\"tbr_flag\"].values\n",
    "        else:\n",
    "            t = np.zeros_like(r)\n",
    "\n",
    "        # run ALS\n",
    "        self.ALS(u, i, r, t)\n",
    "\n",
    "        # optional test eval\n",
    "        if test_data is not None:\n",
    "            preds = self.predict_ratings(test_data)\n",
    "            return math.sqrt(mean_squared_error(test_data[\"rating\"], preds))\n",
    "\n",
    "    def predict_ratings(self, df):\n",
    "        u,i,_ = self.extract_data(df)\n",
    "        out = []\n",
    "        for uu,ii in zip(u,i):\n",
    "            if uu < self.rec_mtx.shape[0] and ii < self.rec_mtx.shape[1]:\n",
    "                out.append(self.rec_mtx[uu,ii])\n",
    "            else:\n",
    "                # fallback to just global or user/item bias\n",
    "                if uu < len(self.user_bias):\n",
    "                    out.append(self.global_mean + self.user_bias[uu])\n",
    "                elif ii < len(self.item_bias):\n",
    "                    out.append(self.global_mean + self.item_bias[ii])\n",
    "                else:\n",
    "                    out.append(self.global_mean)\n",
    "        return np.array(out)\n",
    "\n",
    "    def predict_for_submission(self, sids, pids):\n",
    "        preds = []\n",
    "        for uu,ii in zip(sids, pids):\n",
    "            if uu < self.rec_mtx.shape[0] and ii < self.rec_mtx.shape[1]:\n",
    "                preds.append(self.rec_mtx[uu,ii])\n",
    "            else:\n",
    "                preds.append(self.global_mean)\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 902549, Test set: 225638\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "df_r = load_data(ratings_path)\n",
    "train_data, test_data = train_test_split(df_r, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {len(train_data)}, Test set: {len(test_data)}\")\n",
    "df_t = load_tbr_data(wishlist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters_wishlist(train_data, wishlist_data, val_data, param_grid):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters using grid search\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data: DataFrame with columns ['sid','pid','rating']\n",
    "    val_data: DataFrame with columns ['sid','pid','rating']\n",
    "    param_grid: dict of hyperparameter ranges\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_params: dict of best hyperparameters\n",
    "    best_model: best model\n",
    "    best_rmse: best RMSE score\n",
    "    \"\"\"\n",
    "    best_rmse = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Convert param_grid to list of dictionaries\n",
    "    from itertools import product\n",
    "    keys = param_grid.keys()\n",
    "    values = param_grid.values()\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    \n",
    "    print(f\"Evaluating {len(param_combinations)} hyperparameter combinations\")\n",
    "    \n",
    "    for params in param_combinations:\n",
    "        print(f\"Evaluating parameters: {params}\")\n",
    "        \n",
    "        model = WeightedALSModel(\n",
    "            rank=params['factors'],\n",
    "            num_iterations=params['iterations'],\n",
    "            reg_parameter=params['regularization'],\n",
    "            num_svd_runs=params.get('num_svd_runs', 3),\n",
    "            svd_lr=params.get('svd_lr', 0.1),\n",
    "            use_iSVD=params.get('use_iSVD', False),\n",
    "            transpose=params.get('transpose', False),\n",
    "            bias_reg=params['bias_reg'],\n",
    "            use_bias=True,\n",
    "            use_confidence=True,\n",
    "            alpha_r=params['alpha_r'],\n",
    "            alpha_tbr=params['alpha_tbr']\n",
    "        )\n",
    "         \n",
    "        \n",
    "        val_rmse = model.train(train_data=train_data, tbr_data=wishlist_data, test_data=val_data)\n",
    "        print(f\"Validation RMSE: {val_rmse}\")\n",
    "        \n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "            \n",
    "    return best_params, best_model, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'factors': [40],\n",
    "    'regularization': [0.1,0.2,0.35,0.5,0.6], \n",
    "    'iterations': [1,5,10,50,100],\n",
    "    'num_svd_runs': [3,6],\n",
    "    'svd_lr': [4,5,5.5,6,7,8,9], \n",
    "    'use_iSVD': [False],\n",
    "    'transpose': [False],\n",
    "    'bias_reg': [0.05,0.01,0.2,0.35,0.5, 0.1,0.75,0.8,1.0,1.1,1.2,1.5,2], \n",
    "    'alpha_r': [1,5,10,13,14,15,16,17,20,30,35,40],\n",
    "    'alpha_tbr': [0,0,0.05,0.01,0.1,0.21,5,7,10] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 1 hyperparameter combinations\n",
      "Evaluating parameters: {'factors': 40, 'regularization': 0.6, 'iterations': 1, 'num_svd_runs': 6, 'svd_lr': 5.5, 'use_iSVD': False, 'transpose': False, 'bias_reg': 1.0, 'alpha_r': 15, 'alpha_tbr': 0}\n",
      "Validation RMSE: 0.8657531031562601\n",
      "Best parameters: {'factors': 40, 'regularization': 0.6, 'iterations': 1, 'num_svd_runs': 6, 'svd_lr': 5.5, 'use_iSVD': False, 'transpose': False, 'bias_reg': 1.0, 'alpha_r': 15, 'alpha_tbr': 0}, Best RMSE: 0.8657531031562601\n"
     ]
    }
   ],
   "source": [
    "best_params, best_model, best_rmse = tune_hyperparameters_wishlist(train_data, df_t, test_data, param_grid)\n",
    "print(f\"Best parameters: {best_params}, Best RMSE: {best_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with best parameters\n",
    "model = WeightedALSModel(rank=40, \n",
    "                         num_iterations=1,\n",
    "                         reg_parameter=0.6,\n",
    "                         num_svd_runs= 6,\n",
    "                         svd_lr= 5.5,\n",
    "                         use_iSVD = False,\n",
    "                         transpose = False,\n",
    "                         bias_reg = 1.0,\n",
    "                         alpha_r=15.0, \n",
    "                         alpha_tbr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.8657531031562601\n"
     ]
    }
   ],
   "source": [
    "# optional test\n",
    "rmse = model.train(train_data=train_data, tbr_data=df_t, test_data=test_data)\n",
    "print(\"Test RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to /Users/ccylmichel/Documents/CIL/BayesWatch_CollaborativeFiltering/Data/sample_submissionALS.csv\n"
     ]
    }
   ],
   "source": [
    "make_submission(model,\n",
    "                sample_path=sample_path,\n",
    "                filename=   os.path.join(DATA_DIR, 'sample_submissionALS.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

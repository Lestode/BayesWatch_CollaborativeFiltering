{
    "model_dim": 64,
    "pretrain_epochs": 25,
    "pretrain_lr_gmf": 0.00015,
    "pretrain_lr_mlp": 1e-05,
    "pretrain_batch_size": 256,
    "lr": 0.0002,
    "batch_size": 128,
    "epochs": 80,
    "num_negative_samples_bpr": 30,
    "bpr_weight": 0.3
}